{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) Pipeline\n",
    "\n",
    "This notebook demonstrates the setup and implementation of a Retrieval-Augmented Generation (RAG) pipeline using LangChain, Hugging Face models, and Groq API. The RAG pipeline combines retrieval-based methods with generative models to answer queries based on a given context. The Rag is used to develop a system that, for each natural language question in the BIRD benchmark, identifies \n",
    "the Source Tables (STs) that contain data relevant to answering the question; \n",
    "results are evaluated against the BIRD ground truth computing the overall recall, precision, and F1-score for detected STs. \n",
    "\n",
    "## Setup and Dependencies\n",
    "\n",
    "First, we import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import os\n",
    "from groq import Groq\n",
    "from langchain_groq import ChatGroq\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,pipeline\n",
    "import torch\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the LLM\n",
    "for this project 2 different llms have been used: \n",
    "<ul>\n",
    "<li>phi-2 (2.7b): a smaller model run locally, Phi-2 is a Transformer with 2.7 billion parameters.</li>\n",
    "<li>llama-3.1-8b-instant: accessed through API,  Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture with 8 bilion parameters.</li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## local setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer from your HDD\n",
    "save_directory = \"filepath\\phi-2\" #use the filepath to your model's location\n",
    "model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Hugging Face Pipeline\n",
    "\n",
    "We create a Hugging Face pipeline for text generation using the loaded model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Hugging Face pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    #temperature=0,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap the Pipeline in LangChain's HuggingFacePipeline\n",
    "\n",
    "We wrap the Hugging Face pipeline in LangChain's `HuggingFacePipeline` to integrate it with the LangChain framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the pipeline in LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Setup\n",
    "\n",
    "Alternatively, we can set up an LLM using the Groq API. This requires an API key and initializes the `ChatGroq` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get model from api\n",
    "#  Set up Groq API\n",
    "os.environ[\"GROQ_API_KEY\"] = \"your API key\"  # Replace with your Groq API key\n",
    "# Initialize Groq LLM\n",
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Split Documents\n",
    "\n",
    "We load the documents from a text file and split them into smaller chunks for processing. the document contains the description (in natural language) of the database we are using. This data is split into smaller documents so that the rag can use only usefull parts as context to execute our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2131, which is longer than the specified 1000\n",
      "Created a chunk of size 1013, which is longer than the specified 1000\n",
      "Created a chunk of size 1646, which is longer than the specified 1000\n",
      "Created a chunk of size 1981, which is longer than the specified 1000\n",
      "Created a chunk of size 1282, which is longer than the specified 1000\n",
      "Created a chunk of size 1034, which is longer than the specified 1000\n",
      "Created a chunk of size 3055, which is longer than the specified 1000\n",
      "Created a chunk of size 3438, which is longer than the specified 1000\n",
      "Created a chunk of size 1279, which is longer than the specified 1000\n",
      "Created a chunk of size 1158, which is longer than the specified 1000\n",
      "Created a chunk of size 1283, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "#Load and split documents\n",
    "loader = TextLoader(\"../Data/schema_descriptions.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings\n",
    "\n",
    "We create embeddings for the document chunks using the <b>\"all-MiniLM-L6-v2\"</b> model.\n",
    "This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS.from_documents(texts, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Few-Shot Examples\n",
    "\n",
    "We define a set of few-shot examples to guide the model in answering queries based on the context.\n",
    "These examples serve as a demonstration of the desired input-output format, helping the model understand the task without requiring extensive fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define few-shot examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What is the unabbreviated mailing street address of the school with the highest FRPM count for K-12 students?\",\n",
    "        \"answer\": \"[schools, frpm]\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Who is the top spending customer and how much is the average price per single item purchased by this customer? What currency was being used?\",\n",
    "        \"answer\": \"[customers, transactions_1k, yearmonth]\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the amount spent by customer 38508 at the gas stations? How much had the customer spent in January 2012?\",\n",
    "        \"answer\": \"[transactions_1k, gasstations, yearmonth]\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Please list the lowest three eligible free rates for students aged 5-17 in continuation schools.\",\n",
    "        \"answer\": \"[frpm]\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Few-Shot Examples with Chain-of-Thought Reasoning\n",
    "\n",
    "In this section, we define a set of few-shot examples to guide the model in answering queries based on the context. Additionally, we incorporate **chain-of-thought (CoT)** reasoning to help the model break down the problem step-by-step and generate more accurate and logical responses.\n",
    "\n",
    "This combination of few-shot examples and chain-of-thought reasoning helps the model generalize better and produce more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the example prompt\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=\"Query: {query}\\nAnswer: {answer}\",\n",
    ")\n",
    "\n",
    "# Define the FewShotPromptTemplate with updated instructions\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"\"\"Return ONLY the names of the tables from the context that could be useful for the query. \n",
    "Follow these steps:\n",
    "1. Check if the table name or any of its columns could contain information relevant to the query.\n",
    "2. If the table is relevant, include its name in a list. Do not include column names or explanations.\n",
    "3. Return the result as a Python list of table names in the format: [tablename1, tablename2].\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Examples:\n",
    "\"\"\",\n",
    "    suffix=\"\\n\\nQuery: {query}\\nAnswer:\",\n",
    "    input_variables=[\"context\", \"query\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Retrieval Options\n",
    "\n",
    "We define different retrieval options:<ul><li>single semantic retriever:uses the embeddings to return a set of documents that are both relevant to the query and diverse, improving the quality of retrieved information for downstream tasks like question answering</li><li> ensemble retriever:  combine a sparse retriever with a dense retriever (embedding), because their strengths are complementary. The sparse retriever is good at finding relevant documents based on keywords, while the dense retriever is good at finding relevant documents based on semantic similarity.</li><li> two-step retrieval process: first retrives all the documents relevant to the query, for each initially retrieved document, it finds additional similar documents</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTION 1: single semantic retriver\n",
    "retriever=vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"score_threshold\": 0.7})  # Retrieve all docs with similarity over 0.7\n",
    "#retriever=vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"score_threshold\": 0.5})\n",
    "#retriever=vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTION 2: ensamble retriever, hybrid semantic and kyeword based retriver\n",
    "\n",
    "#semantic retriever\n",
    "retriever=vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3})\n",
    "# Create a keyword-based retriever (BM25)\n",
    "bm25_retriever = BM25Retriever.from_documents(texts)\n",
    "bm25_retriever.k = 3  # Number of documents to retrieve\n",
    "# Combine retrievers with EnsembleRetriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[vector_store.as_retriever(), bm25_retriever],\n",
    "    weights=[0.5, 0.5],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTION 3:\n",
    "#Define the two-step retrieval process\n",
    "def two_step_retrieval(query):\n",
    "    # First retrieval: Retrieve documents relevant to the query\n",
    "    initial_docs = ensemble_retriever.get_relevant_documents(query)\n",
    "\n",
    "    # Second retrieval: Retrieve additional documents similar to the initial ones\n",
    "    additional_docs = []\n",
    "    for doc in initial_docs:\n",
    "        # Find similar documents based on the content of the initial document\n",
    "        similar_docs = vector_store.similarity_search(doc.page_content, k=2)  # Adjust k as needed\n",
    "        additional_docs.extend(similar_docs)\n",
    "\n",
    "    # Combine and deduplicate documents based on page_content\n",
    "    unique_docs = []\n",
    "    seen_contents = set()\n",
    "    for doc in initial_docs + additional_docs:\n",
    "        if doc.page_content not in seen_contents:\n",
    "            unique_docs.append(doc)\n",
    "            seen_contents.add(doc.page_content)\n",
    "\n",
    "    return unique_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the RAG Chain\n",
    "\n",
    "We define the RAG chain using the chosen retriever and the few-shot prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain only semantic retriver\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(lambda x: {\"context\": \"\\n\\n\".join([doc.page_content for doc in x[\"context\"]]), \"query\": x[\"query\"]})\n",
    "    | few_shot_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain hybrid semantich/keyword based\n",
    "rag_chain = (\n",
    "    {\"context\": ensemble_retriever, \"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(lambda x: {\"context\": \"\\n\\n\".join([doc.page_content for doc in x[\"context\"]]), \"query\": x[\"query\"]})\n",
    "    | few_shot_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain for 2 step retrival\n",
    "rag_chain = (\n",
    "    {\"context\": RunnableLambda(two_step_retrieval), \"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(lambda x: {\"context\": \"\\n\\n\".join([doc.page_content for doc in x[\"context\"]]), \"query\": x[\"query\"]})\n",
    "    | few_shot_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[schools, frpm]\n"
     ]
    }
   ],
   "source": [
    "#example query\n",
    "query = \"What is the highest eligible free rate for K-12 students in the schools in Alameda County?\"\n",
    "#query=\"Among the account opened, how many female customers who were born before 1950 and stayed in Sokolov?\"\n",
    "response = rag_chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[schools, frpm]'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[response.rfind(\"[\"):response.rfind(\"]\")+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIRD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
